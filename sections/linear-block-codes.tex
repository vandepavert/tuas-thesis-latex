\documentclass[../main.tex]{subfiles}
\DoIfAndOnlyIfStandAlone{%
    \externaldocument{introduction}%
    \setcounter{chapter}{2}%
}%

\begin{document}

    \chapter{Linear Block Codes}

    \section{Hamming weight, Minimum Distance and Code Rate}
    The Hamming weight $w_H(x)$ of a codeword or vector $x$ is defined as the amount of non-zero elements or vector coordinates, which ranges from zero to length $n$ of said codeword.

    \begin{equation*}
        w_H(x) = \sum_{j=0}^{n-1} w_H(x_j), \text{ where } w_H(x_j) =
        \begin{cases}
            0, &\text{if } x_j = 0 \\
            1, &\text{if } x_j \neq 0
        \end{cases}
    \end{equation*}

    \noindent
    The Hamming distance $d_H(x,y)$ between two codewords or vectors $x$ and $y$ is defined as amount of elements or coordinates where $x$ and $y$ differ.

    \begin{equation*}
        d_H(x,y) = \sum_{j=0}^{n-1} w_H(x_j+y_j), \text{ where } w_H(x_j+y_j) =
        \begin{cases}
            0, \text{if } x_j=y_j \\
            1, \text{if } x_j \neq y_j
        \end{cases}
    \end{equation*}

    \begin{equation*}
        d_H(x,y) =  w_H(x,y)
    \end{equation*}

    \noindent
    The minimum distance $d_min$ of code $C$ is the minimum distance between two different codewords. The minimum distance for linear codes is equal to the minimum weight \autocite{bossert1999channel}. However, a codeword containing only zeros and therefore having a distance of zero is disregarded as the minimum distance cannot be zero.


    Let $x,y$ be codewords in code $C$. A received vector, which is the sent vector $x$ in $C$, plus error vector $e$ can only be corrected if the distance between any other codeword $y$ in $C$ fulfill

    \begin{equation*}
        d_{min}(x, x+e) < d_{min}(y, x+e) \text{ or } w_{min}(e) < w_{min}(x+y+e).
    \end{equation*}

    \noindent
    Therefore

    \begin{equation*}
         w_{min}(e) \leq \frac{d-1}{2},
    \end{equation*}

    \noindent
    where $d$ is the distance. This is written as

    \begin{equation*}
        t \leq \frac{d-1}{2} \text{ or } d \geq 2t+1,
    \end{equation*}

    \noindent
    where $t$ is the amount of errors that can be corrected.

    In general, a code $C$ of length $n$, with $M$ codewords, and a minimum distance $d=d(C)$, is called an $(n,M,d)$ code. Then M$ \leq q^{n-d+1}$ and the coderate of a $q$-ary $(n,M,d)$ code is at most $1-\frac{d-1}{n}$.

    A linear $q$-ary code of lenght $n$, with $k$ codewords or message symbols, and distance $d$ is called a $(n,k,d)$ code or $(n,k)$ code. The code rate is defined as

    \begin{equation*}
        R=\frac{log_qk}{n}.
    \end{equation*}

    \noindent
    If, according to Shannon's channel coding theorem, rate $R$ is less than capacity $C$, then the code exists but if rate $R$ is larger than capacity $C$, the error probability is 1 and the length of the codeword becomes infinite.


    \section{Singleton Bound}
    It is preferable to have a large minimum distance $d$ so that many errors can be corrected. Also, a large amount of codewords $M$ would allow for efficient use of bandwidth when transmitting over a noisy channel. Unfortunately, increasing $d$ tends to increase $n$ or decrease $M$. The Singleton bound is an upper bound for $M$ in terms of $n$ and $d$. A code that satisfies the Singleton bound is called a MDS code (maximum distance separable). The Singleton bound can be written as

    \begin{equation*}
        q^d \leq \frac{q^{n+1}}{M}
    \end{equation*}

    \noindent
    for the MDS code to obtain the largest possible value of $d$ for a given $n$ and $M$. Reed-Solomon codes are an important class of MDS codes \autocite{trappe2006introduction}.


    \section{Maximum-likelihood Decoding}
    There are two principles of decoding. In hard-decision decoding the received bits are believed to be either 1 or 0 in binary transmission. The decoding is done bit by bit. In soft-decision decoding, the received codewords may contain samples of bits with many values, not just 1 or 0. Calculating the closest error-free codeword is more complicated but soft-decision decoding has better performance than the hard-decision decoding.

    Assuming hard-decision decoding is used, the received codeword is decoded into its closest codeword measured by its smallest Hamming distance. This minimum probability of error principle is called Maximum-Likelihood or Minimum Distance Decoding \autocite{geisel1990tutorial}.

    The model of the binary symmetric channel (BSC) \autocite{mackay2003information} in \Cref{fig:binary_symmetric_channel} shows that the channel has binary input and output with an error probability, the channel is characterised by the following conditions if $c$ is the transmitted code and $r$ the received code:

    \begin{align*}
        \begin{rcases*}
            P(r=0|c=0) &= 1-p\\
            P(r=0|c=1) &= p\\
            P(r=1|c=0) &= p\\
            P(r=1|c=1) &= 1-p
        \end{rcases*} \text{ and } 0 \leq p \leq \frac{1}{2}\\
    \end{align*}

    Comparing all received codewords $r$ to all transmitted codewords $c$ as a direct way of correcting errors would not be inefficient. This means storing all $2k$ code vectors and performing equally as many comparisons for each received codeword, resulting in error vectors of which the vector with the smallest distance is probably the transmitted codeword. A more practical decoding method would be Syndrome decoding which will be described in \Cref{sec:syndrome_decoding}.


    \section{Hamming Codes}
    Hamming constructed a code where he added three additional bits to four information bits. These additional or parity bits are chosen based on the information bits in the following manner:

    \begin{align*}
        p_1 &= i_1 + i_2 + i_4\\
        p_2 &= i_1 + i_3 + i_4\\
        p_3 &= i_2 + i_3 + i_4
    \end{align*}

    \noindent
    and form the codeword $c=(i_1, i_2, i_3, i_4, p_1, p_2, p_3)$ . Hamming codes are block codes. This means that a fixed block of input data is processed into a fixed block of output data. A code is called a systematic code if the codeword starts with the information bits, followed by the parity bits, as shown in \Cref{fig:systematic_codeword}. A non-systematic code has the information bits in a different order. The parity bits are the result of a modulo 2 addition, so if there is an even amount of bits, it gives 0 and 1 when there is an odd amount. If a single error occurs, i.e., a bit is flipped or reversed, the codeword no longer satisfies the equations.

    \begin{figure}[htp]
        \centering
        \begin{tikzpicture}[
                set/.style = {draw, circle, minimum size=3cm, align=center},
            ]
            \node (A) at (0,0) [set] {}; \node at (90:0.5cm) {$p_1$};
            \node (B) at (240:1.75cm) [set] {}; \node at (240:2.25cm) {$p_2$};
            \node (C) at (300:1.75cm) [set] {}; \node at (300:2.25cm) {$p_3$};

            \node at (barycentric cs:A=1,B=1) [left] {$i_1$};
            \node at (barycentric cs:A=1,C=1) [right] {$i_2$};
            \node at (barycentric cs:B=1,C=1) [below] {$i_3$};
            \node at (barycentric cs:A=1,B=1,C=1) [] {$i_4$};
        \end{tikzpicture}
        \caption{Relation between information and parity bits}
        \label{fig:information_and_parity_bits}
    \end{figure}

    \begin{figure}[htp]
        \centering
        \begin{tikzpicture}[
                box/.style = {draw, rectangle, minimum size=1cm, align=center},
            ]
            \node (data) [box, minimum width=6cm] {$k$ data bytes};
            \node (parity) [box, right=0cm of data] {$n-k$ parity bytes};
        \end{tikzpicture}
        \caption{An example of a systematic codeword of length $n$}
        \label{fig:systematic_codeword}
    \end{figure}

    \newpage

    \begin{figure}[htp]
        \centering
        \begin{tikzpicture}[
                            > = latex,
                   box/.style = {draw, minimum size=10mm},
                   add/.style = {draw, fill=gray!10, minimum size=10mm},
                   dot/.style = {circle, draw, fill, minimum size=1mm, inner sep=0mm, node contents={}},
            ]
            % data word nodes
            \node (di1) [box] at (0,0)  {$i_1$};
            \node (di2) [box] at (0,-1) {$i_2$};
            \node (di3) [box] at (0,-2) {$i_3$};
            \node (di4) [box] at (0,-3) {$i_4$};

            % code word nodes
            \node (ci1) [box] at (10,0)  {$i_1$};
            \node (ci2) [box] at (10,-1) {$i_2$};
            \node (ci3) [box] at (10,-2) {$i_3$};
            \node (ci4) [box] at (10,-3) {$i_4$};
            \node (cp1) [box] at (10,-4) {$p_1$};
            \node (cp2) [box] at (10,-5) {$p_2$};
            \node (cp3) [box] at (10,-6) {$p_3$};

            % mod 2 adder nodes
            \node (a1) [add] at (5,-4)   {$i_1+i_2+i_4$};
            \node (a2) [add] at (5,-5.5) {$i_1+i_3+i_4$};
            \node (a3) [add] at (5,-7)   {$i_2+i_3+i_4$};

            % connection nodes
            \node (n1) [dot, right=0.5cm of di1];
            \node (n2) [dot, right=1cm of di2];
            \node (n3) [dot, right=1.5cm of di3];
            \node (n4) [dot, right=2cm of di4];
            \node (n5) [left=1cm of cp2] {};
            \node (n6) [left=0.5cm of cp3] {};

            % connections
            \draw (di1) edge (n1); \draw[->] (n1) edge (ci1);
            \draw (di2) edge (n2); \draw[->] (n2) edge (ci2);
            \draw (di3) edge (n3); \draw[->] (n3) edge (ci3);
            \draw (di4) edge (n4); \draw[->] (n4) edge (ci4);
            % i1
            \draw[->] (n1) |- node[dot] {} (a1.south west);
            \draw[->] (n1) |- (a2.south west);
            % i2
            \draw[->] (n2) |- node[dot] {} (a1.west);
            \draw[->] (n2) |- (a3.south west);
            % i3
            \draw[->] (n3) |- node[dot] {} (a2.west);
            \draw[->] (n3) |- (a3.west);
            % i4
            \draw[->] (n4) |- node[dot] {} (a1.north west);
            \draw[->] (n4) |- node[dot] {} (a2.north west);
            \draw[->] (n4) |- (a3.north west);
            % p1
            \draw[->] (a1.east) -- (cp1.west);
            % p2
            \draw[->] (a2.east) -| (n5.center) (n5.center) |- (cp2.west);
            % p3
            \draw[->] (a3.east) -| (n6.center) (n6.center) |- (cp3.west);

            % legends
            \coordinate[above=0.25cm of di1, label={4-bit data word}];
            \coordinate[above=0.25cm of ci1, label={7-bit code word}];
            \coordinate[below=0.5cm of a3, label={modulo 2 adders}];
        \end{tikzpicture}
        \caption{ Hamming (7,4) encoder.}
        \label{fig:hamming_encoder}
    \end{figure}

    The decoder receives a seven-bit codeword $r=(i'_1, i'_2, i'_3, i'_4, p'_1, p'_2, p'_3)$. With an algebraic method known as syndrome decoding it is possible to determine the position of the error:

    \begin{align*}
        s_1 &= p'_1 + i'_1 + i'_2 + i'_4\\
        s_2 &= p'_2 + i'_1 + i'_3 + i'_4\\
        s_3 &= p'_3 + i'_2 + i'_3 + i'_4
    \end{align*}

    \noindent
    The three-bit syndrome $(s_1, s_2, s_3)$ returns $(0, 0, 0)$ when a received codeword contains no errors. There are seven more possible syndromes, each corresponding to the position of the error in the received codeword. The decoder then inverts the detected bit to counter the error.

    \newpage

    \begin{figure}[htp]
        \centering
        \begin{tikzpicture}[
                        > = latex,
                add/.style = {draw, fill=gray!10, minimum size=10mm, align=center},
                box/.style = {draw, minimum size=10mm, align=center},
                dot/.style = {circle, draw, fill, minimum size=1mm, inner sep=0mm, node contents={}},
                sum/.style = {circle, draw, minimum size=5mm,
                            path picture={\draw[thick,shorten <=1.5mm,shorten >=1.5mm,-]
                                            (\ppbb.north) edge (\ppbb.south)
                                            (\ppbb.west)  edge (\ppbb.east);
                                            },% end of path picture /node content/
                            node contents={}},
            ]
            % code word nodes
            \node (ci1) [box] at (0,0)  {$i'_1$};
            \node (ci2) [box] at (0,-1) {$i'_2$};
            \node (ci3) [box] at (0,-2) {$i'_3$};
            \node (ci4) [box] at (0,-3) {$i'_4$};
            \node (cp1) [box] at (0,-4) {$p'_1$};
            \node (cp2) [box] at (0,-5) {$p'_2$};
            \node (cp3) [box] at (0,-6) {$p'_3$};

            % data word nodes
            \node (di1) [box] at (10,0)  {$i_1$};
            \node (di2) [box] at (10,-1) {$i_2$};
            \node (di3) [box] at (10,-2) {$i_3$};
            \node (di4) [box] at (10,-3) {$i_4$};

            % mod 2 adder nodes
            \node (a1) [add, minimum height=1.5cm] at (5.5,-4.5)   {error at\\$i_1, i_2, i_4$\\or $p_1$};
            \node (a2) [add, minimum height=1.5cm] at (5.5,-6.5) {error at\\$i_1, i_3, i_4$\\or $p_2$};
            \node (a3) [add, minimum height=1.5cm] at (5.5,-8.5)   {error at\\$i_2, i_3, i_4$\\or $p_3$};

            % error correction nodes
            \node (e) [add, minimum height=5cm] at (8,-6.5) {error\\correction\\logic};
            \node (s1) [sum, left=0.5cm of di1];
            \node (s2) [sum, left=1cm of di2];
            \node (s3) [sum, left=1.5cm of di3];
            \node (s4) [sum, left=2cm of di4];

            % connection nodes
            \node (n1) [dot, right=2cm of ci1];
            \node (n2) [dot, right=2.5cm of ci2];
            \node (n3) [dot, right=3cm of ci3];
            \node (n4) [dot, right=3.5cm of ci4];
            \node (n5) [right=1.5cm of cp1] {};
            \node (n6) [right=1cm of cp2] {};
            \node (n7) [right=0.5cm of cp3] {};

            % connections
            \draw (ci1) edge (n1); \draw[->] (n1) edge (s1) (s1) edge (di1);
            \draw (ci2) edge (n2); \draw[->] (n2) edge (s2) (s2) edge (di2);
            \draw (ci3) edge (n3); \draw[->] (n3) edge (s3) (s3) edge (di3);
            \draw (ci4) edge (n4); \draw[->] (n4) edge (s4) (s4) edge (di4);
            % i'1
            \draw[->] (n1) |- node[dot] {} \findmid{0.667}{a1.south west}{a1.west};
            \draw[->] (n1) |- \findmid{0.667}{a2.south west}{a2.west};
            % i'2
            \draw[->] (n2) |- node[dot] {} \findmid{0.667}{a1.north west}{a1.west};
            \draw[->] (n2) |- \findmid{0.667}{a3.south west}{a3.west};
            % i'3
            \draw[->] (n3) |- node[dot] {} \findmid{0.667}{a2.north west}{a2.west};
            \draw[->] (n3) |- \findmid{0.667}{a3.north west}{a3.west};
            % i'4
            \draw[->] (n4) |- node[dot] {} (a1.north west);
            \draw[->] (n4) |- node[dot] {} (a2.north west);
            \draw[->] (n4) |- (a3.north west);
            % p'1
            \draw [->] (cp1.east) |- (n5.center) (n5.center) |- (a1.south west);
            % p'2
            \draw [->] (cp2.east) |- (n6.center) (n6.center) |- (a2.south west);
            % p'3
            \draw[->] (cp3.east) |- (n7.center) (n7.center) |- (a3.south west);
            % s1
            \draw[->] (a1.east) node[above right]{$s_1$} to (\tikztostart -| e.west);
            % s2
            \draw[->] (a2.east) node[above right]{$s_2$} to (\tikztostart -| e.west);
            % s3
            \draw[->] (a3.east) node[above right]{$s_3$} to (\tikztostart -| e.west);
            % e1
            \draw[<-] (s1.south) to (\tikztostart |- e.north);
            % e2
            \draw[<-] (s2.south) to (\tikztostart |- e.north);
            % e3
            \draw[<-] (s3.south) to (\tikztostart |- e.north);
            % e4
            \draw[<-] (s4.south) to (\tikztostart |- e.north);

            % legends
            \coordinate[above=0.25cm of di1, label={Decoded 4-bit data word}];
            \coordinate[above=0.25cm of ci1, label={Received 7-bit code word}];
            \coordinate[below=0.5cm of a3, label={modulo 2 adders}];
        \end{tikzpicture}
        \caption{ Hamming (7,4) decoder.}
        \label{fig:hamming_decoder}
    \end{figure}

    \autocite{bose2008information} considered the space of $q$-ary $m$-tuples, where every $q$-ary vector of length $m$ can be represented by its endpoint in this space. Hence, we can represent every codeword as a point in this space, and all codewords at a Hamming distance of $t$ or less would lie within the sphere centred at the codeword and with a radius of $t$.

    \begin{figure}[h!tp]
        \centering
        \begin{tikzpicture}[
                         > = latex,
             sphere/.style = {draw, circle, minimum size=3cm, align=center},
                dot/.style = {circle, draw, fill, minimum size=1mm, inner sep=0mm, node contents={}},
            ]
            % nodes
            \node (c1) at (0,0) [sphere] {}; \node at (c1.center) [dot] {}; \node [right=0.1cm of c1.center] {$c_1$};
            \node (c2) at (5,0) [sphere] {}; \node at (c2.center) [dot] {}; \node [right=0.1cm of c2.center] {$c_2$};
            \node (a) [below=1cm of c1] {};
            \node (b) [below=1cm of c2] {};

            % lines
            \draw[dotted] (a.center) -- (c1.center);
            \draw[dotted] (b.center) -- (c2.center);
            \draw[->] (c1.center) -- node[above right]{$t$} (c1.north west);
            \draw[->] (c2.center) -- node[above right]{$t$} (c2.north west);
            \draw[<->] (a.center) -- node[above]{$d > 2t + 1$} (b.center);

            \end{tikzpicture}
        \caption{Decoding sphere}
        \label{fig:decoding_sphere}
    \end{figure}


    \section{Syndrome Decoding} \label{sec:syndrome_decoding}
    \autocite{trappe2006introduction} define linear $(n,k)$ code of dimension $k$ and length $n$ over a field $F$ as a $k$-dimensional subspace of $F^n$. For example, a linear binary code, of length $n$ and dimension $k$ is a set of $2^k$ binary codewords or $n$-tuples, such that the sum of any two codewords is always a codeword. To construct such a linear $(n,k)$ code, we choose a $k \times n$ matrix known as generator matrix. The rows have to be linearly independent to produce unique codewords.

    Generator matrix $G$ is taken so that $G=[I_k,P]$, where $I_k$ is the $k \times k$ identity matrix which determine the codewords and $P$ is a $k \times (n-k)$ matrix that provides redundancy, the parity matrix. Now every codeword $c$ of code $C$ can be expressed as a linear combination of rows of $G$ by $c=i \cdot G$. We can now calculate the generator matrix for a systematic representation. For example, a systematic Hamming (7,4) code has the following generator matrix:

    \begin{equation*}
        G=[I_4|P]=
        \begin{bmatrix}
            1&0&0&0&1&1&0\\
            0&1&0&0&1&0&1\\
            0&0&1&0&0&1&1\\
            0&0&0&1&1&1&1
        \end{bmatrix}
    \end{equation*}

    \noindent
    The parity check matrix is then calculated as

    \begin{equation*}
        H=[-P^T|I_3]=
        \begin{bmatrix}
            1&1&0&1&1&0&0\\
            1&0&1&1&0&1&0\\
            0&1&1&1&0&0&1
        \end{bmatrix}
    \end{equation*}

    \noindent
    For example, encoding information bits $[1 1 0 0]$ gives

    \begin{equation*}
        \begin{bmatrix}
            1&1&0&0
        \end{bmatrix}
        \times
        \begin{bmatrix}
            1&0&0&0&1&1&0\\
            0&1&0&0&1&0&1\\
            0&0&1&0&0&1&1\\
            0&0&0&1&1&1&1
        \end{bmatrix}
        \equiv
        \begin{bmatrix}
            1&1&0&0&0&1&1
        \end{bmatrix}
    \end{equation*}

    \noindent
    Decoding received codeword $c'=[1100011]$ with syndrome decoding results in $[000]$ when no errors are detected. However, in our example an error was introduced in the fifth position of codeword $c'=[1100111]$, so we can expect a syndrome with non-zero elements.

    \begin{equation*}
        c' \times H^T =
        \begin{bmatrix}
            1&1&0&0&\underline{1}&1&1
        \end{bmatrix}
        \times
        \begin{bmatrix}
            1&1&0\\
            1&0&1\\
            0&1&1\\
            1&1&1\\
            \underline{1}&\underline{0}&\underline{0}\\
            0&1&0\\
            0&0&1
        \end{bmatrix}
        \equiv
        \begin{bmatrix}
            1&0&0
        \end{bmatrix}
    \end{equation*}

    \noindent
    The value $[100]$ can be looked up in parity check matrix $H$ and tells that the error occurred in the fifth position from the left. Correction based on syndrome requires more steps and asks for a matrix of all single error vectors. Codeword $c=[1100011]$ and received codeword $c=[1100111]$ give an error vector of $e=[0000100]$ or $c=c'+e$. Since we already know that $s=c' \cdot H^T$ and an error-free $c \cdot H^T$ has a syndrome with all zero elements, we now substitute $c'$ with $c+e$ because $c=c'+e$ is equivalent to $c'=c+e$ in binary.

    \begin{align*}
        s &= c' \cdot H^T\\
          &= (c+e) \cdot H^T\\
          &= c \cdot H^T + e \cdot H^T\\
          &= 0 + e \cdot H^T\\
          &= e \cdot H^T
    \end{align*}

    \noindent
    We can conclude that the syndrome solely depends on the error pattern and not on the transmitted codeword.


    \section{Cyclic Codes}


    \section{BCH Codes}


    \section{Generating BCH Code}


    \section{Decoding BCH Code}


\end{document}



